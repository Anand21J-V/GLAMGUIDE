{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-06-25T08:39:39.333885Z",
     "iopub.status.busy": "2024-06-25T08:39:39.333464Z",
     "iopub.status.idle": "2024-06-25T08:39:39.341228Z",
     "shell.execute_reply": "2024-06-25T08:39:39.340079Z",
     "shell.execute_reply.started": "2024-06-25T08:39:39.333854Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-06-25T08:39:50.523913Z",
     "iopub.status.busy": "2024-06-25T08:39:50.523527Z",
     "iopub.status.idle": "2024-06-25T08:39:50.529301Z",
     "shell.execute_reply": "2024-06-25T08:39:50.527860Z",
     "shell.execute_reply.started": "2024-06-25T08:39:50.523884Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data_dir = '/kaggle/input/fashion-product-images-small/'\n",
    "images_folder = os.path.join(data_dir, 'images')\n",
    "styles_path = os.path.join(data_dir, 'styles.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-06-25T08:41:15.758108Z",
     "iopub.status.busy": "2024-06-25T08:41:15.757716Z",
     "iopub.status.idle": "2024-06-25T08:41:15.863912Z",
     "shell.execute_reply": "2024-06-25T08:41:15.862823Z",
     "shell.execute_reply.started": "2024-06-25T08:41:15.758078Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read styles.csv for labels\n",
    "df = pd.read_csv(styles_path, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-06-25T08:42:36.513271Z",
     "iopub.status.busy": "2024-06-25T08:42:36.512938Z",
     "iopub.status.idle": "2024-06-25T08:42:37.176230Z",
     "shell.execute_reply": "2024-06-25T08:42:37.174945Z",
     "shell.execute_reply.started": "2024-06-25T08:42:36.513245Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['id'].apply(lambda x: os.path.isfile(os.path.join(images_folder, str(x) + '.jpg')))]\n",
    "df['image'] = df['id'].apply(lambda x: os.path.join(images_folder, str(x) + '.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-06-25T08:42:54.731628Z",
     "iopub.status.busy": "2024-06-25T08:42:54.731177Z",
     "iopub.status.idle": "2024-06-25T08:42:54.813633Z",
     "shell.execute_reply": "2024-06-25T08:42:54.812503Z",
     "shell.execute_reply.started": "2024-06-25T08:42:54.731592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subCategory\n",
      "Topwear                     15398\n",
      "Shoes                        7343\n",
      "Bags                         3055\n",
      "Bottomwear                   2693\n",
      "Watches                      2542\n",
      "Innerwear                    1808\n",
      "Jewellery                    1079\n",
      "Eyewear                      1073\n",
      "Fragrance                    1011\n",
      "Sandal                        963\n",
      "Wallets                       933\n",
      "Flip Flops                    913\n",
      "Belts                         811\n",
      "Socks                         698\n",
      "Lips                          527\n",
      "Dress                         478\n",
      "Loungewear and Nightwear      470\n",
      "Saree                         427\n",
      "Nails                         329\n",
      "Makeup                        307\n",
      "Headwear                      293\n",
      "Ties                          258\n",
      "Accessories                   129\n",
      "Scarves                       118\n",
      "Cufflinks                     108\n",
      "Apparel Set                   106\n",
      "Free Gifts                    104\n",
      "Stoles                         90\n",
      "Skin Care                      77\n",
      "Skin                           69\n",
      "Eyes                           43\n",
      "Mufflers                       38\n",
      "Shoe Accessories               24\n",
      "Sports Equipment               21\n",
      "Gloves                         20\n",
      "Hair                           19\n",
      "Bath and Body                  12\n",
      "Water Bottle                    7\n",
      "Perfumes                        6\n",
      "Umbrellas                       6\n",
      "Beauty Accessories              4\n",
      "Wristbands                      4\n",
      "Sports Accessories              3\n",
      "Home Furnishing                 1\n",
      "Vouchers                        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Focus on the 'subCategory' column\n",
    "df = df[['image', 'subCategory']]\n",
    "\n",
    "# Count the number of samples in each category\n",
    "category_counts = df['subCategory'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# Filter categories with at least 800 samples\n",
    "valid_categories = category_counts[category_counts >= 800].index\n",
    "\n",
    "# Filter the dataframe to include only valid categories\n",
    "df = df[df['subCategory'].isin(valid_categories)]\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.1, stratify=df['subCategory'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['subCategory'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-06-25T08:42:58.443308Z",
     "iopub.status.busy": "2024-06-25T08:42:58.442923Z",
     "iopub.status.idle": "2024-06-25T08:43:21.771360Z",
     "shell.execute_reply": "2024-06-25T08:43:21.770163Z",
     "shell.execute_reply.started": "2024-06-25T08:42:58.443277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35659 validated image filenames belonging to 13 classes.\n",
      "Found 1981 validated image filenames belonging to 13 classes.\n",
      "Found 1982 validated image filenames belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generators for training and validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=30,        \n",
    "    width_shift_range=0.1,   \n",
    "    height_shift_range=0.1,   \n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,          \n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)  # No augmentation for test set\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='image',\n",
    "    y_col='subCategory',\n",
    "    target_size=(160, 120), \n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='image',\n",
    "    y_col='subCategory',\n",
    "    target_size=(160, 120),  \n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='image',\n",
    "    y_col='subCategory',\n",
    "    target_size=(160, 120),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-06-25T08:43:56.374998Z",
     "iopub.status.busy": "2024-06-25T08:43:56.374626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1229s\u001b[0m 2s/step - accuracy: 0.5348 - loss: 2.9766 - val_accuracy: 0.7259 - val_loss: 1.8306 - learning_rate: 1.0000e-04\n",
      "Epoch 2/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1206s\u001b[0m 2s/step - accuracy: 0.6868 - loss: 2.0276 - val_accuracy: 0.8420 - val_loss: 1.5408 - learning_rate: 1.0000e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1213s\u001b[0m 2s/step - accuracy: 0.7356 - loss: 1.7733 - val_accuracy: 0.7224 - val_loss: 1.8237 - learning_rate: 1.0000e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1211s\u001b[0m 2s/step - accuracy: 0.7643 - loss: 1.6465 - val_accuracy: 0.8950 - val_loss: 1.2695 - learning_rate: 1.0000e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 2s/step - accuracy: 0.7962 - loss: 1.4833 - val_accuracy: 0.8440 - val_loss: 1.3398 - learning_rate: 1.0000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1233s\u001b[0m 2s/step - accuracy: 0.8096 - loss: 1.3926 - val_accuracy: 0.8672 - val_loss: 1.2262 - learning_rate: 1.0000e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1238s\u001b[0m 2s/step - accuracy: 0.8148 - loss: 1.3116 - val_accuracy: 0.9268 - val_loss: 1.0319 - learning_rate: 1.0000e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1234s\u001b[0m 2s/step - accuracy: 0.8242 - loss: 1.2226 - val_accuracy: 0.7683 - val_loss: 1.3802 - learning_rate: 1.0000e-04\n",
      "Epoch 9/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1233s\u001b[0m 2s/step - accuracy: 0.8304 - loss: 1.1628 - val_accuracy: 0.8975 - val_loss: 0.9684 - learning_rate: 1.0000e-04\n",
      "Epoch 10/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1234s\u001b[0m 2s/step - accuracy: 0.8389 - loss: 1.1025 - val_accuracy: 0.9192 - val_loss: 0.9197 - learning_rate: 1.0000e-04\n",
      "Epoch 11/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 2s/step - accuracy: 0.8509 - loss: 1.0382 - val_accuracy: 0.9313 - val_loss: 0.8224 - learning_rate: 1.0000e-04\n",
      "Epoch 12/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1244s\u001b[0m 2s/step - accuracy: 0.8521 - loss: 1.0074 - val_accuracy: 0.9213 - val_loss: 0.8916 - learning_rate: 1.0000e-04\n",
      "Epoch 13/25\n",
      "\u001b[1m558/558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1272s\u001b[0m 2s/step - accuracy: 0.8533 - loss: 0.9640 - val_accuracy: 0.8955 - val_loss: 0.9176 - learning_rate: 1.0000e-04\n",
      "Epoch 14/25\n",
      "\u001b[1m 41/558\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:41\u001b[0m 2s/step - accuracy: 0.8697 - loss: 0.9237"
     ]
    }
   ],
   "source": [
    "# Create a custom CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(160, 120, 3)),\n",
    "    BatchNormalization(), \n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)), \n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5), \n",
    "    Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for improved training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
    "class_weights = dict(zip(np.unique(train_generator.classes), class_weights))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    class_weight=class_weights  # Add class weights\n",
    ")\n",
    "'''\n",
    "# Save model using .keras format\n",
    "model.save('model.keras')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Predictions on Test Set\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Get true classes and class labels\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(val_generator.class_indices.keys())\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, y_pred, target_names=class_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_classes, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(class_labels))\n",
    "plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "plt.yticks(tick_marks, class_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(class_labels)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 175990,
     "sourceId": 396802,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
